"""
Îç∞Ïù¥ÌÑ∞ ÌååÏù¥ÌîÑÎùºÏù∏ Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞
- PDF ‚Üí ÌååÏã± ‚Üí Ï≤≠ÌÇπ ‚Üí ÏûÑÎ≤†Îî© ‚Üí Î≤°ÌÑ∞DB Ï†ÑÏ≤¥ ÌîåÎ°úÏö∞ Í¥ÄÎ¶¨
"""

import json
import time
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import asdict

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.utils.pdf_parser import PDFParser, PDFDocument
from src.utils.chunker import DocumentChunker, Chunk
from src.utils.metadata_extractor import MetadataExtractor, DocumentMetadata
from src.services.embedding_service import EmbeddingService
from src.services.vector_store import MultiCollectionVectorStore


class DocumentProcessingPipeline:
    """
    Î¨∏ÏÑú Ï≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏

    Pipeline:
    1. PDF ÌååÏã±
    2. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
    3. Ï≤≠ÌÇπ
    4. ÏûÑÎ≤†Îî© ÏÉùÏÑ±
    5. Î≤°ÌÑ∞ DB Ï†ÄÏû•
    """

    def __init__(
        self,
        embedding_service: EmbeddingService,
        vector_store: MultiCollectionVectorStore,
        output_dir: str = "data/processed"
    ):
        """
        Args:
            embedding_service: ÏûÑÎ≤†Îî© ÏÑúÎπÑÏä§
            vector_store: Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥
            output_dir: Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
        """
        self.pdf_parser = PDFParser(preserve_layout=True)
        self.metadata_extractor = MetadataExtractor()
        self.chunker = DocumentChunker(
            chunk_size=1000,
            chunk_overlap=200,
            min_chunk_size=100,
            max_chunk_size=2000
        )
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def process_document(
        self,
        pdf_path: str,
        chunking_strategy: str = "recursive",
        save_intermediate: bool = True
    ) -> Dict:
        """
        Îã®Ïùº Î¨∏ÏÑú Ï≤òÎ¶¨

        Args:
            pdf_path: PDF ÌååÏùº Í≤ΩÎ°ú
            chunking_strategy: Ï≤≠ÌÇπ Ï†ÑÎûµ
            save_intermediate: Ï§ëÍ∞Ñ Í≤∞Í≥º Ï†ÄÏû• Ïó¨Î∂Ä

        Returns:
            Ï≤òÎ¶¨ Í≤∞Í≥º ÌÜµÍ≥Ñ
        """
        start_time = time.time()
        pdf_path = Path(pdf_path)

        print(f"\n{'='*60}")
        print(f"üìÑ Processing: {pdf_path.name}")
        print(f"{'='*60}\n")

        # 1. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
        print("1Ô∏è‚É£  Extracting metadata...")
        doc_metadata = self.metadata_extractor.extract_from_filename(str(pdf_path))
        print(f"   - Document ID: {doc_metadata.document_id}")
        print(f"   - Type: {doc_metadata.type}")
        print(f"   - Language: {doc_metadata.language}")

        # 2. PDF ÌååÏã±
        print("\n2Ô∏è‚É£  Parsing PDF...")
        pdf_document = self.pdf_parser.parse(str(pdf_path), extract_images=False)
        print(f"   - Pages: {pdf_document.total_pages}")
        print(f"   - Language: {pdf_document.language}")

        # Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
        full_text = "\n\n".join([page.text for page in pdf_document.pages])

        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï
        content_metadata = self.metadata_extractor.extract_from_content(
            full_text,
            doc_metadata
        )

        # 3. Ï≤≠ÌÇπ
        print(f"\n3Ô∏è‚É£  Chunking with strategy: {chunking_strategy}...")
        base_metadata = {
            "document_id": doc_metadata.document_id,
            "type": doc_metadata.type,
            "language": doc_metadata.language,
            "version": doc_metadata.version,
            "source_file": doc_metadata.source_file,
        }

        chunks = self.chunker.chunk_document(
            text=full_text,
            metadata=base_metadata,
            strategy=chunking_strategy
        )
        print(f"   - Generated {len(chunks)} chunks")
        print(f"   - Avg chunk size: {sum(c.char_count for c in chunks) // len(chunks)} chars")

        # Ï§ëÍ∞Ñ Í≤∞Í≥º Ï†ÄÏû•
        if save_intermediate:
            self._save_chunks(doc_metadata.document_id, chunks)

        # 4. ÏûÑÎ≤†Îî© ÏÉùÏÑ±
        print(f"\n4Ô∏è‚É£  Generating embeddings...")
        chunk_texts = [chunk.text for chunk in chunks]
        embeddings = self.embedding_service.embed_batch(
            texts=chunk_texts,
            batch_size=50,
            show_progress=True
        )
        print(f"   - Generated {len(embeddings)} embeddings")

        # 5. Î≤°ÌÑ∞ DB Ï†ÄÏû•
        print(f"\n5Ô∏è‚É£  Saving to vector database...")

        # Î≤°ÌÑ∞ DBÏö© Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        vector_chunks = []
        for chunk, embedding in zip(chunks, embeddings):
            vector_chunk = {
                "chunk_id": chunk.chunk_id,
                "text": chunk.text,
                "embedding": embedding,
                "metadata": chunk.metadata
            }
            vector_chunks.append(vector_chunk)

        # Î¨∏ÏÑú ÌÉÄÏûÖÍ≥º Ïñ∏Ïñ¥Ïóê ÎßûÎäî Ïª¨Î†âÏÖòÏóê Ï†ÄÏû•
        doc_type_short = doc_metadata.type.replace("_contact", "").replace("_memo", "").replace("_fulfillment", "").replace("_master", "")
        if doc_type_short == "account":
            doc_type_short = "account"
        elif doc_type_short == "meeting":
            doc_type_short = "meeting"
        elif doc_type_short == "order":
            doc_type_short = "order"
        elif doc_type_short == "common":
            doc_type_short = "common"

        lang_code = "ko" if doc_metadata.language == "korean" else "en"

        self.vector_store.add_document_chunks(
            document_type=doc_type_short,
            language=lang_code,
            chunks=vector_chunks,
            batch_size=100
        )

        # Ï≤òÎ¶¨ ÏãúÍ∞Ñ
        elapsed_time = time.time() - start_time

        # Í≤∞Í≥º ÌÜµÍ≥Ñ
        stats = {
            "document_id": doc_metadata.document_id,
            "source_file": doc_metadata.source_file,
            "type": doc_metadata.type,
            "language": doc_metadata.language,
            "total_pages": pdf_document.total_pages,
            "total_chunks": len(chunks),
            "total_chars": len(full_text),
            "processing_time_seconds": round(elapsed_time, 2),
            "collection_name": f"crm_{doc_type_short}_{lang_code}"
        }

        print(f"\n‚úÖ Processing completed in {elapsed_time:.2f}s")
        print(f"   - Saved to collection: crm_{doc_type_short}_{lang_code}")

        return stats

    def process_folder(
        self,
        folder_path: str,
        chunking_strategy: str = "recursive",
        file_pattern: str = "*.pdf"
    ) -> List[Dict]:
        """
        Ìè¥Îçî ÎÇ¥ Î™®Îì† PDF Ï≤òÎ¶¨

        Args:
            folder_path: PDF Ìè¥Îçî Í≤ΩÎ°ú
            chunking_strategy: Ï≤≠ÌÇπ Ï†ÑÎûµ
            file_pattern: ÌååÏùº Ìå®ÌÑ¥

        Returns:
            Í∞Å Î¨∏ÏÑúÏùò Ï≤òÎ¶¨ Í≤∞Í≥º ÌÜµÍ≥Ñ Î¶¨Ïä§Ìä∏
        """
        folder = Path(folder_path)
        pdf_files = list(folder.glob(file_pattern))

        print(f"\n{'='*60}")
        print(f"üìÅ Processing folder: {folder}")
        print(f"   Found {len(pdf_files)} PDF files")
        print(f"{'='*60}")

        all_stats = []
        errors = []

        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"\n[{i}/{len(pdf_files)}]")
            try:
                stats = self.process_document(
                    pdf_path=str(pdf_file),
                    chunking_strategy=chunking_strategy,
                    save_intermediate=True
                )
                all_stats.append(stats)
            except Exception as e:
                error_info = {
                    "file": pdf_file.name,
                    "error": str(e)
                }
                errors.append(error_info)
                print(f"\n‚ùå Error processing {pdf_file.name}: {e}")

        # ÏµúÏ¢Ö ÏöîÏïΩ
        print(f"\n{'='*60}")
        print(f"üìä Processing Summary")
        print(f"{'='*60}")
        print(f"‚úÖ Successfully processed: {len(all_stats)}")
        print(f"‚ùå Failed: {len(errors)}")
        if all_stats:
            total_chunks = sum(s["total_chunks"] for s in all_stats)
            total_time = sum(s["processing_time_seconds"] for s in all_stats)
            print(f"üì¶ Total chunks: {total_chunks}")
            print(f"‚è±Ô∏è  Total time: {total_time:.2f}s")

        if errors:
            print(f"\n‚ùå Errors:")
            for error in errors:
                print(f"   - {error['file']}: {error['error']}")

        # Í≤∞Í≥º Ï†ÄÏû•
        self._save_processing_report(all_stats, errors)

        return all_stats

    def _save_chunks(self, document_id: str, chunks: List[Chunk]):
        """Ï≤≠ÌÅ¨Î•º JSON ÌååÏùºÎ°ú Ï†ÄÏû•"""
        output_file = self.output_dir / f"{document_id}_chunks.json"

        chunks_data = []
        for chunk in chunks:
            chunk_data = {
                "chunk_id": chunk.chunk_id,
                "text": chunk.text,
                "metadata": chunk.metadata,
                "char_count": chunk.char_count,
                "token_count": chunk.token_count
            }
            chunks_data.append(chunk_data)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)

        print(f"   üíæ Saved chunks to: {output_file}")

    def _save_processing_report(self, stats: List[Dict], errors: List[Dict]):
        """Ï≤òÎ¶¨ Î¶¨Ìè¨Ìä∏ Ï†ÄÏû•"""
        report_file = self.output_dir / "processing_report.json"

        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_documents": len(stats) + len(errors),
            "successful": len(stats),
            "failed": len(errors),
            "statistics": stats,
            "errors": errors
        }

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nüíæ Report saved to: {report_file}")


def create_pipeline(
    openai_api_key: str,
    qdrant_host: str = "localhost",
    qdrant_port: int = 6333,
    vector_size: int = 3072,
    use_memory: bool = False
) -> DocumentProcessingPipeline:
    """
    ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉùÏÑ± Ìó¨Ìçº Ìï®Ïàò

    Args:
        openai_api_key: OpenAI API ÌÇ§
        qdrant_host: Qdrant Ìò∏Ïä§Ìä∏
        qdrant_port: Qdrant Ìè¨Ìä∏
        vector_size: Î≤°ÌÑ∞ Ï∞®Ïõê
        use_memory: Î©îÎ™®Î¶¨ Î™®Îìú ÏÇ¨Ïö© (Docker ÏóÜÏùÑ Îïå)

    Returns:
        DocumentProcessingPipeline Ïù∏Ïä§ÌÑ¥Ïä§
    """
    # ÏûÑÎ≤†Îî© ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî
    embedding_service = EmbeddingService(
        model_name="openai/text-embedding-3-large",
        api_key=openai_api_key,
        cache_enabled=True
    )

    # Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Ï¥àÍ∏∞Ìôî (ÏûêÎèôÏúºÎ°ú Ïó∞Í≤∞ ÏãúÎèÑ)
    try:
        if use_memory:
            raise ConnectionError("Memory mode requested")

        vector_store = MultiCollectionVectorStore(
            host=qdrant_host,
            port=qdrant_port
        )
        print("‚úÖ Connected to Qdrant server")
    except Exception as e:
        print(f"‚ö†Ô∏è  Cannot connect to Qdrant server: {e}")
        print("‚ö†Ô∏è  Using in-memory mode (data will not persist)")
        from src.services.vector_store import VectorStore
        # Î©îÎ™®Î¶¨ Î™®ÎìúÎ°ú Ìè¥Î∞±
        vector_store_memory = VectorStore(use_memory=True)
        # MultiCollectionVectorStore Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú ÎûòÌïë
        class MemoryMultiCollectionVectorStore:
            def __init__(self, store):
                self.store = store
                self.collections = {}

            def initialize_crm_collections(self, vector_size, recreate=False):
                for doc_type in ["account", "meeting", "order", "common"]:
                    for lang in ["ko", "en"]:
                        collection_name = f"crm_{doc_type}_{lang}"
                        self.store.create_collection(
                            collection_name=collection_name,
                            vector_size=vector_size,
                            recreate=recreate
                        )
                        self.collections[collection_name] = True

            def add_document_chunks(self, document_type, language, chunks, batch_size=100):
                collection_name = f"crm_{document_type}_{language}"
                self.store.add_documents(
                    collection_name=collection_name,
                    chunks=chunks,
                    batch_size=batch_size
                )

        vector_store = MemoryMultiCollectionVectorStore(vector_store_memory)

    # Ïª¨Î†âÏÖò ÏÉùÏÑ±
    vector_store.initialize_crm_collections(
        vector_size=vector_size,
        recreate=False  # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ Ïú†ÏßÄ
    )

    # ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉùÏÑ±
    pipeline = DocumentProcessingPipeline(
        embedding_service=embedding_service,
        vector_store=vector_store
    )

    return pipeline


if __name__ == "__main__":
    # ÌÖåÏä§Ìä∏ ÏΩîÎìú
    import os
    from dotenv import load_dotenv

    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        print("‚ùå OPENAI_API_KEY not found in environment")
        exit(1)

    # ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉùÏÑ±
    pipeline = create_pipeline(
        openai_api_key=api_key,
        qdrant_host="localhost",
        qdrant_port=6333
    )

    # ÌÖåÏä§Ìä∏: Îã®Ïùº ÌååÏùº Ï≤òÎ¶¨
    if len(sys.argv) > 1:
        pdf_path = sys.argv[1]
        if Path(pdf_path).is_file():
            stats = pipeline.process_document(pdf_path)
            print(f"\n‚úÖ Processing complete!")
            print(json.dumps(stats, indent=2, ensure_ascii=False))
        elif Path(pdf_path).is_dir():
            stats = pipeline.process_folder(pdf_path)
        else:
            print(f"‚ùå Path not found: {pdf_path}")
    else:
        print("Usage: python pipeline.py <pdf_file_or_folder>")
