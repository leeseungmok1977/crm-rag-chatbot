"""
ë²¡í„° DB ì—°ë™ ëª¨ë“ˆ
- Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤
- CRUD ì‘ì—…
- ê²€ìƒ‰ ê¸°ëŠ¥
"""

from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import uuid

from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance,
    VectorParams,
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue,
    SearchParams,
)
from tqdm import tqdm


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    chunk_id: str
    text: str
    score: float
    metadata: Dict


class VectorStore:
    """
    Qdrant ë²¡í„° ìŠ¤í† ì–´ í´ë˜ìŠ¤

    Features:
    - ì»¬ë ‰ì…˜ ìƒì„±/ì‚­ì œ
    - ë²¡í„° ì €ì¥
    - ìœ ì‚¬ë„ ê²€ìƒ‰
    - ë©”íƒ€ë°ì´í„° í•„í„°ë§
    """

    def __init__(
        self,
        host: str = "localhost",
        port: int = 6333,
        api_key: Optional[str] = None,
        use_memory: bool = False
    ):
        """
        Args:
            host: Qdrant í˜¸ìŠ¤íŠ¸
            port: Qdrant í¬íŠ¸
            api_key: API í‚¤ (í´ë¼ìš°ë“œ ì‚¬ìš© ì‹œ)
            use_memory: ë©”ëª¨ë¦¬ ëª¨ë“œ (í…ŒìŠ¤íŠ¸ìš©)
        """
        if use_memory:
            self.client = QdrantClient(":memory:")
            print("âœ… Vector store initialized (in-memory mode)")
        else:
            self.client = QdrantClient(
                host=host,
                port=port,
                api_key=api_key
            )
            print(f"âœ… Vector store connected to {host}:{port}")

    def create_collection(
        self,
        collection_name: str,
        vector_size: int,
        distance: str = "Cosine",
        recreate: bool = False
    ):
        """
        ì»¬ë ‰ì…˜ ìƒì„±

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            vector_size: ë²¡í„° ì°¨ì›
            distance: ê±°ë¦¬ ë©”íŠ¸ë¦­ (Cosine, Euclid, Dot)
            recreate: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
        """
        # ê±°ë¦¬ ë©”íŠ¸ë¦­ ë§¤í•‘
        distance_map = {
            "Cosine": Distance.COSINE,
            "Euclid": Distance.EUCLID,
            "Dot": Distance.DOT,
        }

        if recreate and self.collection_exists(collection_name):
            self.client.delete_collection(collection_name)
            print(f"ğŸ—‘ï¸  Deleted existing collection: {collection_name}")

        if not self.collection_exists(collection_name):
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=distance_map.get(distance, Distance.COSINE)
                )
            )
            print(f"âœ… Created collection: {collection_name} (dim={vector_size})")
        else:
            print(f"â„¹ï¸  Collection already exists: {collection_name}")

    def collection_exists(self, collection_name: str) -> bool:
        """ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        collections = self.client.get_collections().collections
        return any(c.name == collection_name for c in collections)

    def add_documents(
        self,
        collection_name: str,
        chunks: List[Dict],  # {chunk_id, text, embedding, metadata}
        batch_size: int = 100,
        show_progress: bool = True
    ):
        """
        ë¬¸ì„œ ì²­í¬ ì¶”ê°€

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            show_progress: ì§„í–‰ë¥  í‘œì‹œ
        """
        if not chunks:
            print("âš ï¸  No chunks to add")
            return

        print(f"ğŸ“¥ Adding {len(chunks)} chunks to {collection_name}")

        # ë°°ì¹˜ ì²˜ë¦¬
        batches = [
            chunks[i:i + batch_size]
            for i in range(0, len(chunks), batch_size)
        ]

        iterator = tqdm(batches, desc="Uploading") if show_progress else batches

        for batch in iterator:
            points = []
            for chunk in batch:
                point = PointStruct(
                    id=str(uuid.uuid4()),  # ê³ ìœ  ID
                    vector=chunk["embedding"],
                    payload={
                        "chunk_id": chunk["chunk_id"],
                        "text": chunk["text"],
                        **chunk.get("metadata", {})
                    }
                )
                points.append(point)

            self.client.upsert(
                collection_name=collection_name,
                points=points
            )

        print(f"âœ… Added {len(chunks)} chunks")

    def search(
        self,
        collection_name: str,
        query_vector: List[float],
        top_k: int = 5,
        filters: Optional[Dict] = None,
        score_threshold: Optional[float] = None
    ) -> List[SearchResult]:
        """
        ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            query_vector: ì¿¼ë¦¬ ë²¡í„°
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼
            filters: ë©”íƒ€ë°ì´í„° í•„í„° (ì˜ˆ: {"type": "account_contact"})
            score_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # í•„í„° ìƒì„±
        query_filter = None
        if filters:
            conditions = []
            for key, value in filters.items():
                conditions.append(
                    FieldCondition(
                        key=key,
                        match=MatchValue(value=value)
                    )
                )
            query_filter = Filter(must=conditions)

        # ê²€ìƒ‰
        search_result = self.client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=top_k,
            query_filter=query_filter,
            score_threshold=score_threshold,
            with_payload=True,
            with_vectors=False
        )

        # ê²°ê³¼ ë³€í™˜
        results = []
        for hit in search_result:
            payload = hit.payload
            result = SearchResult(
                chunk_id=payload.get("chunk_id", ""),
                text=payload.get("text", ""),
                score=hit.score,
                metadata={k: v for k, v in payload.items()
                         if k not in ["chunk_id", "text"]}
            )
            results.append(result)

        return results

    def search_by_filters(
        self,
        collection_name: str,
        filters: Dict,
        limit: int = 100
    ) -> List[Dict]:
        """
        ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ ì—†ì´)

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            filters: ë©”íƒ€ë°ì´í„° í•„í„°
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        conditions = []
        for key, value in filters.items():
            conditions.append(
                FieldCondition(
                    key=key,
                    match=MatchValue(value=value)
                )
            )

        query_filter = Filter(must=conditions)

        results = self.client.scroll(
            collection_name=collection_name,
            scroll_filter=query_filter,
            limit=limit,
            with_payload=True,
            with_vectors=False
        )

        documents = []
        for point in results[0]:
            documents.append(point.payload)

        return documents

    def delete_documents(
        self,
        collection_name: str,
        filters: Dict
    ):
        """
        ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ë¬¸ì„œ ì‚­ì œ

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            filters: ë©”íƒ€ë°ì´í„° í•„í„°
        """
        conditions = []
        for key, value in filters.items():
            conditions.append(
                FieldCondition(
                    key=key,
                    match=MatchValue(value=value)
                )
            )

        query_filter = Filter(must=conditions)

        self.client.delete(
            collection_name=collection_name,
            points_selector=query_filter
        )
        print(f"ğŸ—‘ï¸  Deleted documents matching filters: {filters}")

    def get_collection_info(self, collection_name: str) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        info = self.client.get_collection(collection_name)
        return {
            "name": collection_name,
            "vectors_count": info.vectors_count,
            "points_count": info.points_count,
            "status": info.status,
            "config": {
                "vector_size": info.config.params.vectors.size,
                "distance": info.config.params.vectors.distance.name
            }
        }

    def list_collections(self) -> List[str]:
        """ëª¨ë“  ì»¬ë ‰ì…˜ ì´ë¦„ ì¡°íšŒ"""
        collections = self.client.get_collections().collections
        return [c.name for c in collections]

    def delete_collection(self, collection_name: str):
        """ì»¬ë ‰ì…˜ ì‚­ì œ"""
        self.client.delete_collection(collection_name)
        print(f"ğŸ—‘ï¸  Deleted collection: {collection_name}")


class MultiCollectionVectorStore:
    """
    ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì„ ê´€ë¦¬í•˜ëŠ” ë²¡í„° ìŠ¤í† ì–´

    CRM ë§¤ë‰´ì–¼ì˜ ê²½ìš° 8ê°œ ì»¬ë ‰ì…˜ ê´€ë¦¬:
    - crm_account_ko, crm_account_en
    - crm_meeting_ko, crm_meeting_en
    - crm_order_ko, crm_order_en
    - crm_common_ko, crm_common_en
    """

    def __init__(
        self,
        host: str = "localhost",
        port: int = 6333,
        api_key: Optional[str] = None
    ):
        self.store = VectorStore(host=host, port=port, api_key=api_key)

    def initialize_crm_collections(
        self,
        vector_size: int,
        recreate: bool = False
    ):
        """
        CRM ë§¤ë‰´ì–¼ìš© 8ê°œ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”

        Args:
            vector_size: ë²¡í„° ì°¨ì›
            recreate: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
        """
        doc_types = ["account", "meeting", "order", "common"]
        languages = ["ko", "en"]

        print(f"ğŸ—ï¸  Initializing CRM collections (vector_size={vector_size})")

        for doc_type in doc_types:
            for lang in languages:
                collection_name = f"crm_{doc_type}_{lang}"
                self.store.create_collection(
                    collection_name=collection_name,
                    vector_size=vector_size,
                    distance="Cosine",
                    recreate=recreate
                )

    def add_document_chunks(
        self,
        document_type: str,  # account, meeting, order, common
        language: str,       # ko, en
        chunks: List[Dict],
        batch_size: int = 100
    ):
        """
        íŠ¹ì • ë¬¸ì„œ íƒ€ì…/ì–¸ì–´ì˜ ì²­í¬ ì¶”ê°€

        Args:
            document_type: ë¬¸ì„œ íƒ€ì…
            language: ì–¸ì–´
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        collection_name = f"crm_{document_type}_{language}"
        self.store.add_documents(
            collection_name=collection_name,
            chunks=chunks,
            batch_size=batch_size
        )

    def search_all_collections(
        self,
        query_vector: List[float],
        top_k: int = 5,
        language: Optional[str] = None,
        doc_type: Optional[str] = None
    ) -> List[SearchResult]:
        """
        ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ í›„ ê²°í•©

        Args:
            query_vector: ì¿¼ë¦¬ ë²¡í„°
            top_k: ì»¬ë ‰ì…˜ë‹¹ ìƒìœ„ Kê°œ (ì´ ê²°ê³¼ëŠ” ë” ë§ì„ ìˆ˜ ìˆìŒ)
            language: ì–¸ì–´ í•„í„° (Noneì´ë©´ ëª¨ë“  ì–¸ì–´)
            doc_type: ë¬¸ì„œ íƒ€ì… í•„í„° (Noneì´ë©´ ëª¨ë“  íƒ€ì…)

        Returns:
            í†µí•© ê²€ìƒ‰ ê²°ê³¼ (ì ìˆ˜ ìˆœ ì •ë ¬)
        """
        all_results = []

        # ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ ê²°ì •
        doc_types = [doc_type] if doc_type else ["account", "meeting", "order", "common"]
        languages = [language] if language else ["ko", "en"]

        for dt in doc_types:
            for lang in languages:
                collection_name = f"crm_{dt}_{lang}"

                if self.store.collection_exists(collection_name):
                    results = self.store.search(
                        collection_name=collection_name,
                        query_vector=query_vector,
                        top_k=top_k
                    )
                    all_results.extend(results)

        # ì ìˆ˜ ìˆœ ì •ë ¬
        all_results.sort(key=lambda x: x.score, reverse=True)

        return all_results[:top_k * 2]  # ìµœì¢… ìƒìœ„ ê²°ê³¼ ë°˜í™˜

    def get_all_stats(self) -> Dict[str, Dict]:
        """ëª¨ë“  ì»¬ë ‰ì…˜ í†µê³„"""
        stats = {}

        for collection_name in self.store.list_collections():
            if collection_name.startswith("crm_"):
                try:
                    stats[collection_name] = self.store.get_collection_info(collection_name)
                except:
                    stats[collection_name] = {"error": "Failed to get info"}

        return stats


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def setup_crm_vector_store(
    host: str = "localhost",
    port: int = 6333,
    vector_size: int = 3072,  # OpenAI text-embedding-3-large
    recreate: bool = False
) -> MultiCollectionVectorStore:
    """
    CRM RAG ì±—ë´‡ìš© ë²¡í„° ìŠ¤í† ì–´ ì…‹ì—…

    Args:
        host: Qdrant í˜¸ìŠ¤íŠ¸
        port: Qdrant í¬íŠ¸
        vector_size: ë²¡í„° ì°¨ì›
        recreate: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±

    Returns:
        MultiCollectionVectorStore ì¸ìŠ¤í„´ìŠ¤
    """
    store = MultiCollectionVectorStore(host=host, port=port)
    store.initialize_crm_collections(vector_size=vector_size, recreate=recreate)
    return store


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("=== Vector Store Test ===\n")

    # ë©”ëª¨ë¦¬ ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸
    store = VectorStore(use_memory=True)

    # ì»¬ë ‰ì…˜ ìƒì„±
    store.create_collection(
        collection_name="test_collection",
        vector_size=768,
        recreate=True
    )

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_chunks = [
        {
            "chunk_id": "chunk_001",
            "text": "ê±°ë˜ì„  ë“±ë¡ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.",
            "embedding": [0.1] * 768,
            "metadata": {
                "type": "account_contact",
                "language": "korean",
                "page": 10
            }
        },
        {
            "chunk_id": "chunk_002",
            "text": "ë¯¸íŒ…ë©”ëª¨ ì‘ì„± ê°€ì´ë“œì…ë‹ˆë‹¤.",
            "embedding": [0.2] * 768,
            "metadata": {
                "type": "meeting_memo",
                "language": "korean",
                "page": 25
            }
        }
    ]

    # ë¬¸ì„œ ì¶”ê°€
    store.add_documents(
        collection_name="test_collection",
        chunks=test_chunks,
        show_progress=False
    )

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query_vector = [0.15] * 768
    results = store.search(
        collection_name="test_collection",
        query_vector=query_vector,
        top_k=2,
        filters={"language": "korean"}
    )

    print("\n=== Search Results ===")
    for i, result in enumerate(results, 1):
        print(f"\n{i}. Score: {result.score:.4f}")
        print(f"   Chunk ID: {result.chunk_id}")
        print(f"   Text: {result.text}")
        print(f"   Metadata: {result.metadata}")

    # ì»¬ë ‰ì…˜ ì •ë³´
    info = store.get_collection_info("test_collection")
    print(f"\n=== Collection Info ===")
    print(f"Points: {info['points_count']}")
    print(f"Vector size: {info['config']['vector_size']}")
